"""
æ¨¡å‹åŒ…è£…å™¨ - æ ¹æ®æ¶æ„è®¾è®¡æ›´æ–°
æ”¯æŒ Gemini, OpenAI, Anthropic
"""
import os
import time
import random
from typing import List, Dict, Any, Optional, Union
from abc import ABC, abstractmethod
from dotenv import load_dotenv
from loguru import logger
import json
import hashlib

# å¯¼å…¥é¢„ç®—ç›‘æ§
try:
    from .budget_monitor import get_budget_monitor
except ImportError:
    # å¦‚æœé¢„ç®—ç›‘æ§æ¨¡å—ä¸å¯ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£
    def get_budget_monitor():
        class DummyMonitor:
            def add_cost(self, cost, model_name="unknown"):
                return True
        return DummyMonitor()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥Google Gemini
try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logger.warning("Google Gemini SDK æœªå®‰è£…")

# å¯¼å…¥OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI SDK æœªå®‰è£…: pip install openai")

# å¯¼å…¥Anthropic
try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic SDK æœªå®‰è£…: pip install anthropic")


class BaseModelWrapper(ABC):
    """åŸºç¡€æ¨¡å‹åŒ…è£…å™¨æ¥å£"""
    
    def __init__(self, model_name: str, temperature: float = 0.7):
        self.model_name = model_name
        self.temperature = temperature
        self._cache = {}
        self.total_tokens_used = 0
        self.total_cost = 0.0
        
    @abstractmethod
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """ç”Ÿæˆå“åº”"""
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°"""
        pass
    
    def get_cache_key(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{system_prompt or ''}{prompt}{self.temperature}{self.model_name}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "model": self.model_name,
            "total_tokens": self.total_tokens_used,
            "total_cost": self.total_cost,
            "cache_size": len(self._cache)
        }
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self._cache.clear()
        logger.debug(f"ç¼“å­˜å·²æ¸…é™¤ï¼ŒåŸå¤§å°: {len(self._cache)}")


# æ¨¡å‹æ³¨å†Œè¡¨å’Œæ³¨å†Œè£…é¥°å™¨
MODEL_REGISTRY = {}

def register_model(name):
    def decorator(cls):
        MODEL_REGISTRY[name] = cls
        return cls
    return decorator

def create_model(model_type: str = "gemini", **kwargs):
    if model_type not in MODEL_REGISTRY:
        raise ValueError(f"Unknown model type: {model_type}")
    return MODEL_REGISTRY[model_type](**kwargs)


@register_model("gemini")
class GeminiWrapper(BaseModelWrapper):
    """Gemini æ¨¡å‹åŒ…è£…å™¨"""
    
    MODELS = {
        "flash": "gemini-2.0-flash-exp",  # æ¨èAgentä½¿ç”¨
        "flash-thinking": "gemini-2.0-flash-thinking-exp-1219",  # æ€è€ƒæ¨¡å‹
        "pro": "gemini-1.5-pro-002",
        "pro-latest": "gemini-exp-1206",
        "pro-2.5": "gemini-2.5-pro",  # æœ€æ–°Gemini 2.5 Pro
    }
    
    def __init__(self, model_type: str = "flash", temperature: float = 0.7):
        model_name = self.MODELS.get(model_type, self.MODELS["flash"])
        super().__init__(model_name, temperature)
        
        if not GEMINI_AVAILABLE:
            raise ImportError("è¯·å®‰è£… google-generativeai: pip install google-generativeai")
        
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® GOOGLE_API_KEY ç¯å¢ƒå˜é‡")
        
        genai.configure(api_key=api_key)
        
        self.generation_config = {
            "temperature": temperature,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 4096,
        }
        
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=self.generation_config,
            safety_settings=self.safety_settings
        )
        
        logger.info(f"åˆå§‹åŒ– Gemini æ¨¡å‹: {self.model_name}")
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        cache_key = self.get_cache_key(prompt, system_prompt)
        if cache_key in self._cache:
            logger.debug("ä½¿ç”¨ç¼“å­˜å“åº”")
            return self._cache[cache_key]
        
        full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        
        try:
            response = self.model.generate_content(full_prompt)
            if response.text:
                result = response.text.strip()
                self._cache[cache_key] = result
                
                # æ›´æ–°ç»Ÿè®¡
                tokens = self.count_tokens(full_prompt) + self.count_tokens(result)
                self.total_tokens_used += tokens
                
                return result
        except Exception as e:
            logger.error(f"Gemini ç”Ÿæˆå¤±è´¥: {e}")
            raise
        
        return ""
    
    def count_tokens(self, text: str) -> int:
        try:
            return self.model.count_tokens(text).total_tokens
        except:
            return len(text) // 4


# class OpenAIWrapper(BaseModelWrapper):
#     """OpenAI æ¨¡å‹åŒ…è£…å™¨"""
    
#     MODELS = {
#         "gpt-4o": "gpt-4o-2024-11-20",  # æœ€æ–°GPT-4o
#         "gpt-4o-mini": "gpt-4o-mini",  # å¿«é€Ÿä¾¿å®œç‰ˆæœ¬
#         "gpt-4-turbo": "gpt-4-turbo-2024-04-09",  # GPT-4 Turbo
#         "gpt-4-128k": "gpt-4-1106-preview",  # 128kä¸Šä¸‹æ–‡
#     }
    
#     # ä»·æ ¼ï¼ˆæ¯1K tokensï¼‰
#     PRICING = {
#         "gpt-4o-2024-11-20": {"input": 0.0025, "output": 0.01},
#         "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
#         "gpt-4-turbo-2024-04-09": {"input": 0.01, "output": 0.03},
#         "gpt-4-1106-preview": {"input": 0.01, "output": 0.03},
#     }
@register_model("openai")
class OpenAIWrapper(BaseModelWrapper):
    """OpenAI æ¨¡å‹åŒ…è£…å™¨"""
    
    MODELS = {
        "gpt-4o": "gpt-4o",  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹å
        "gpt-4o-mini": "gpt-4o-mini",  # å¿«é€Ÿä¾¿å®œç‰ˆæœ¬
        "gpt-4-turbo": "gpt-4-turbo",  # GPT-4 Turbo
        "gpt-4": "gpt-4",  # æ ‡å‡† GPT-4
        "gpt-3.5-turbo": "gpt-3.5-turbo",  # GPT-3.5
    }
    
    # ä»·æ ¼ï¼ˆæ¯1K tokensï¼‰
    PRICING = {
        "gpt-4o": {"input": 0.005, "output": 0.015},
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "gpt-4-turbo": {"input": 0.01, "output": 0.03},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    }
    
    def __init__(self, model_type: str = "gpt-4o", temperature: float = 0.7):
        model_name = self.MODELS.get(model_type, model_type)
        super().__init__(model_name, temperature)
        
        if not OPENAI_AVAILABLE:
            raise ImportError("è¯·å®‰è£… openai: pip install openai")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        self.client = OpenAI(api_key=api_key)
        
        # é€Ÿç‡é™åˆ¶é…ç½®
        self.max_requests_per_minute = int(os.getenv("MAX_REQUESTS_PER_MINUTE", "60"))
        self.rate_limit_retry_attempts = int(os.getenv("RATE_LIMIT_RETRY_ATTEMPTS", "3"))
        self.request_timestamps = []
        
        logger.info(f"åˆå§‹åŒ– OpenAI æ¨¡å‹: {self.model_name} (é€Ÿç‡é™åˆ¶: {self.max_requests_per_minute}/åˆ†é’Ÿ)")
    
    def _check_rate_limit(self):
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„æ—¶é—´æˆ³
        self.request_timestamps = [ts for ts in self.request_timestamps if current_time - ts < 60]
        
        if len(self.request_timestamps) >= self.max_requests_per_minute:
            oldest_timestamp = min(self.request_timestamps)
            wait_time = 60 - (current_time - oldest_timestamp) + 1
            logger.warning(f"é€Ÿç‡é™åˆ¶: å·²è¾¾åˆ° {self.max_requests_per_minute}/åˆ†é’Ÿï¼Œç­‰å¾… {wait_time:.1f}ç§’")
            time.sleep(wait_time)
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        cache_key = self.get_cache_key(prompt, system_prompt)
        if cache_key in self._cache:
            logger.debug("ä½¿ç”¨ç¼“å­˜å“åº”")
            return self._cache[cache_key]
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # é€Ÿç‡é™åˆ¶æ£€æŸ¥
        self._check_rate_limit()
        
        # é‡è¯•é€»è¾‘
        for attempt in range(self.rate_limit_retry_attempts + 1):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=4096
                )
                
                # è®°å½•è¯·æ±‚æ—¶é—´æˆ³
                self.request_timestamps.append(time.time())
                break
                
            except Exception as e:
                if "rate_limit" in str(e).lower() and attempt < self.rate_limit_retry_attempts:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)  # æŒ‡æ•°é€€é¿
                    logger.warning(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{self.rate_limit_retry_attempts})")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"OpenAI ç”Ÿæˆå¤±è´¥: {e}")
                    raise
        
        result = response.choices[0].message.content.strip()
        self._cache[cache_key] = result
        
        # æ›´æ–°ç»Ÿè®¡
        if response.usage:
            self.total_tokens_used += response.usage.total_tokens
            
            # è®¡ç®—æˆæœ¬
            if self.model_name in self.PRICING:
                pricing = self.PRICING[self.model_name]
                input_cost = (response.usage.prompt_tokens / 1000) * pricing["input"]
                output_cost = (response.usage.completion_tokens / 1000) * pricing["output"]
                total_cost = input_cost + output_cost
                self.total_cost += total_cost
                
                # è¯¦ç»†tokenæ—¥å¿—
                if os.getenv("LOG_TOKENS", "true").lower() == "true":
                    logger.info(f"ğŸ“Š {self.model_name} Tokenä½¿ç”¨: "
                              f"prompt={response.usage.prompt_tokens}, "
                              f"completion={response.usage.completion_tokens}, "
                              f"total={response.usage.total_tokens}, "
                              f"cost=${total_cost:.6f}")
                
                # é¢„ç®—ç›‘æ§
                budget_monitor = get_budget_monitor()
                if not budget_monitor.add_cost(total_cost, self.model_name):
                    raise RuntimeError("é¢„ç®—è¶…é™ï¼Œåœæ­¢æ‰§è¡Œ")
        
        return result
    
    def count_tokens(self, text: str) -> int:
        # ä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®è®¡ç®—ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        return len(text) // 4


class AnthropicWrapper(BaseModelWrapper):
    """Anthropic Claude æ¨¡å‹åŒ…è£…å™¨"""
    
    MODELS = {
        "claude-3.5-sonnet": "claude-3-5-sonnet-20241022",  # æœ€å¼º
        "claude-3.5-haiku": "claude-3-5-haiku-20241022",  # å¿«é€Ÿä¾¿å®œ
        "claude-3-opus": "claude-3-opus-20240229",  # ä¹‹å‰çš„æœ€å¼º
    }
    
    # ä»·æ ¼ï¼ˆæ¯1K tokensï¼‰
    PRICING = {
        "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
        "claude-3-5-haiku-20241022": {"input": 0.00025, "output": 0.00125},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
    }
    
    def __init__(self, model_type: str = "claude-3.5-sonnet", temperature: float = 0.7):
        model_name = self.MODELS.get(model_type, model_type)
        super().__init__(model_name, temperature)
        
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("è¯·å®‰è£… anthropic: pip install anthropic")
        
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
        
        self.client = Anthropic(api_key=api_key)
        logger.info(f"åˆå§‹åŒ– Anthropic æ¨¡å‹: {self.model_name}")
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        cache_key = self.get_cache_key(prompt, system_prompt)
        if cache_key in self._cache:
            logger.debug("ä½¿ç”¨ç¼“å­˜å“åº”")
            return self._cache[cache_key]
        
        try:
            message = self.client.messages.create(
                model=self.model_name,
                max_tokens=4096,
                temperature=self.temperature,
                system=system_prompt if system_prompt else "You are a helpful assistant.",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            result = message.content[0].text.strip()    
            self._cache[cache_key] = result
            
            # æ›´æ–°ç»Ÿè®¡
            if hasattr(message, 'usage'):
                input_tokens = message.usage.input_tokens
                output_tokens = message.usage.output_tokens
                self.total_tokens_used += input_tokens + output_tokens
                
                # è®¡ç®—æˆæœ¬
                if self.model_name in self.PRICING:
                    pricing = self.PRICING[self.model_name]
                    input_cost = (input_tokens / 1000) * pricing["input"]
                    output_cost = (output_tokens / 1000) * pricing["output"]
                    self.total_cost += input_cost + output_cost
            
            return result
            
        except Exception as e:
            logger.error(f"Anthropic ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def count_tokens(self, text: str) -> int:
        # Claudeçš„tokenè®¡ç®—è¿‘ä¼¼
        return len(text) // 3


# ===== æ ¹æ®æ¶æ„å›¾åˆ›å»ºç‰¹å®šçš„æ¨¡å‹å®ä¾‹ =====

def create_recommendation_agent() -> BaseModelWrapper:
    """
    åˆ›å»ºæ¨èæ™ºèƒ½ä½“
    æ ¹æ®æ¶æ„ï¼šGemini 2.0 Flash æˆ– GPT-4o Mini
    """
    # è°ƒè¯•ç¯å¢ƒå˜é‡
    openai_key = os.getenv("OPENAI_API_KEY")
    google_key = os.getenv("GOOGLE_API_KEY")
    
    logger.info(f"Environment check - OpenAI: {'âœ…' if openai_key else 'âŒ'}, Google: {'âœ…' if google_key else 'âŒ'}")
    
    # ä¼˜å…ˆä½¿ç”¨ Gemini Flashï¼ˆå…è´¹ï¼‰
    if google_key:
        try:
            logger.info("ä½¿ç”¨ Gemini 2.0 Flash ä½œä¸ºæ¨èæ¨¡å‹")
            return GeminiWrapper(model_type="flash", temperature=0.7)
        except Exception as e:
            logger.warning(f"Gemini Flash ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šGPT-4o Miniï¼ˆä¾¿å®œï¼‰
    if openai_key:
        try:
            logger.info("ä½¿ç”¨ GPT-4o Mini ä½œä¸ºæ¨èæ¨¡å‹")
            return OpenAIWrapper(model_type="gpt-4o-mini", temperature=0.7)
        except Exception as e:
            logger.warning(f"GPT-4o Mini ä¸å¯ç”¨: {e}")
    
    # è¯¦ç»†é”™è¯¯ä¿¡æ¯
    error_msg = "æ²¡æœ‰å¯ç”¨çš„æ¨èæ¨¡å‹ - "
    if not openai_key and not google_key:
        error_msg += "OPENAI_API_KEY å’Œ GOOGLE_API_KEY éƒ½æœªè®¾ç½®"
    elif not openai_key:
        error_msg += "OPENAI_API_KEY æœªè®¾ç½®"
    elif not google_key:
        error_msg += "GOOGLE_API_KEY æœªè®¾ç½®"
    
    raise RuntimeError(error_msg)


# def create_evaluation_agent() -> BaseModelWrapper:
#     """
#     åˆ›å»ºè¯„ä¼°æ™ºèƒ½ä½“
#     æ ¹æ®æ¶æ„ï¼šGPT-4o (128k) æˆ– Claude 3.5 Haiku
#     """
#     # ä¼˜å…ˆä½¿ç”¨ GPT-4oï¼ˆå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼‰
#     if os.getenv("OPENAI_API_KEY"):
#         try:
#             logger.info("ä½¿ç”¨ GPT-4o (128k) ä½œä¸ºè¯„ä¼°æ¨¡å‹")
#             return OpenAIWrapper(model_type="gpt-4-128k", temperature=0.3)
#         except Exception as e:
#             logger.warning(f"GPT-4o ä¸å¯ç”¨: {e}")
    
#     # å¤‡é€‰ï¼šClaude 3.5 Haikuï¼ˆä¾¿å®œä¸”å¿«é€Ÿï¼‰
#     if os.getenv("ANTHROPIC_API_KEY"):
#         try:
#             logger.info("ä½¿ç”¨ Claude 3.5 Haiku ä½œä¸ºè¯„ä¼°æ¨¡å‹")
#             return AnthropicWrapper(model_type="claude-3.5-haiku", temperature=0.3)
#         except Exception as e:
#             logger.warning(f"Claude 3.5 Haiku ä¸å¯ç”¨: {e}")
    
#     # æœ€åé€‰æ‹©ï¼šä½¿ç”¨æ ‡å‡† GPT-4o
#     if os.getenv("OPENAI_API_KEY"):
#         try:
#             logger.info("ä½¿ç”¨æ ‡å‡† GPT-4o ä½œä¸ºè¯„ä¼°æ¨¡å‹")
#             return OpenAIWrapper(model_type="gpt-4o", temperature=0.3)
#         except Exception as e:
#             logger.warning(f"GPT-4o ä¸å¯ç”¨: {e}")
    
#     raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°æ¨¡å‹")
def create_evaluation_agent() -> BaseModelWrapper:
    """
    åˆ›å»ºè¯„ä¼°æ™ºèƒ½ä½“
    ä½¿ç”¨å¯ç”¨çš„æœ€ä½³æ¨¡å‹ - ä¼˜å…ˆä½¿ç”¨æœ€æ–°æ¨¡å‹
    """
    # ä¼˜å…ˆä½¿ç”¨ Gemini 2.5 Proï¼ˆæœ€æ–°ï¼‰
    if os.getenv("GOOGLE_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ Gemini 2.5 Pro ä½œä¸ºè¯„ä¼°æ¨¡å‹")
            return GeminiWrapper(model_type="pro-2.5", temperature=0.3)
        except Exception as e:
            logger.warning(f"Gemini 2.5 Pro ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šGPT-4o
    if os.getenv("OPENAI_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ GPT-4o ä½œä¸ºè¯„ä¼°æ¨¡å‹")
            return OpenAIWrapper(model_type="gpt-4o", temperature=0.3)
        except Exception as e:
            logger.warning(f"GPT-4o ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šä½¿ç”¨æ ‡å‡† GPT-4
    if os.getenv("OPENAI_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ GPT-4 ä½œä¸ºè¯„ä¼°æ¨¡å‹")
            return OpenAIWrapper(model_type="gpt-4", temperature=0.3)
        except Exception as e:
            logger.warning(f"GPT-4 ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šClaude 3.5 Haiku
    if os.getenv("ANTHROPIC_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ Claude 3.5 Haiku ä½œä¸ºè¯„ä¼°æ¨¡å‹")
            return AnthropicWrapper(model_type="claude-3.5-haiku", temperature=0.3)
        except Exception as e:
            logger.warning(f"Claude 3.5 Haiku ä¸å¯ç”¨: {e}")
    
    # æœ€åé€‰æ‹©ï¼šä½¿ç”¨ GPT-3.5-turbo
    if os.getenv("OPENAI_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ GPT-3.5-turbo ä½œä¸ºè¯„ä¼°æ¨¡å‹")
            return OpenAIWrapper(model_type="gpt-3.5-turbo", temperature=0.3)
        except Exception as e:
            logger.warning(f"GPT-3.5-turbo ä¸å¯ç”¨: {e}")
    
    raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°æ¨¡å‹")


def create_optimizer_agent() -> BaseModelWrapper:
    """
    åˆ›å»ºä¼˜åŒ–æ™ºèƒ½ä½“
    æ ¹æ®æ¶æ„ï¼šä¼˜å…ˆä½¿ç”¨æœ€æ–°æ¨¡å‹
    """
    # ä¼˜å…ˆä½¿ç”¨ Claude 3.5 Sonnetï¼ˆåˆ›æ„/åˆ†æå¼ºï¼Œæˆæœ¬ä½40%ï¼‰
    if os.getenv("ANTHROPIC_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ Claude 3.5 Sonnet ä½œä¸ºä¼˜åŒ–æ¨¡å‹")
            return AnthropicWrapper(model_type="claude-3.5-sonnet", temperature=0.5)
        except Exception as e:
            logger.warning(f"Claude 3.5 Sonnet ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šGemini 2.5 Proï¼ˆæœ€æ–°ï¼‰
    if os.getenv("GOOGLE_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ Gemini 2.5 Pro ä½œä¸ºä¼˜åŒ–æ¨¡å‹")
            return GeminiWrapper(model_type="pro-2.5", temperature=0.5)
        except Exception as e:
            logger.warning(f"Gemini 2.5 Pro ä¸å¯ç”¨: {e}")
    
    # å¤‡é€‰ï¼šGPT-4o
    if os.getenv("OPENAI_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ GPT-4o ä½œä¸ºä¼˜åŒ–æ¨¡å‹")
            return OpenAIWrapper(model_type="gpt-4o", temperature=0.5)
        except Exception as e:
            logger.warning(f"GPT-4o ä¸å¯ç”¨: {e}")
    
    # æœ€åé€‰æ‹©ï¼šGemini Flash Thinking
    if os.getenv("GOOGLE_API_KEY"):
        try:
            logger.info("ä½¿ç”¨ Gemini Flash Thinking ä½œä¸ºä¼˜åŒ–æ¨¡å‹")
            return GeminiWrapper(model_type="flash-thinking", temperature=0.5)
        except Exception as e:
            logger.warning(f"Gemini Flash Thinking ä¸å¯ç”¨: {e}")
    
    raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ä¼˜åŒ–æ¨¡å‹")


# è¾…åŠ©å‡½æ•°ï¼šæ‰“å°æˆæœ¬ç»Ÿè®¡
def print_cost_summary(agents: Dict[str, BaseModelWrapper]):
    """æ‰“å°æ‰€æœ‰agentçš„æˆæœ¬ç»Ÿè®¡"""
    total_cost = 0
    total_tokens = 0
    
    logger.info("\n" + "="*50)
    logger.info("ğŸ“Š æˆæœ¬ç»Ÿè®¡")
    logger.info("="*50)
    
    for name, agent in agents.items():
        stats = agent.get_stats()
        logger.info(f"\n{name}:")
        logger.info(f"  æ¨¡å‹: {stats['model']}")
        logger.info(f"  Tokens: {stats['total_tokens']:,}")
        logger.info(f"  æˆæœ¬: ${stats['total_cost']:.4f}")
        
        total_cost += stats['total_cost']
        total_tokens += stats['total_tokens']
    
    logger.info(f"\næ€»è®¡:")
    logger.info(f"  æ€»Tokens: {total_tokens:,}")
    logger.info(f"  æ€»æˆæœ¬: ${total_cost:.4f}")
    logger.info("="*50)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logger.info("æµ‹è¯•æ¨¡å‹é…ç½®...")
    
    # æ£€æŸ¥APIå¯†é’¥
    keys = {
        "Google": bool(os.getenv("GOOGLE_API_KEY")),
        "OpenAI": bool(os.getenv("OPENAI_API_KEY")),
        "Anthropic": bool(os.getenv("ANTHROPIC_API_KEY"))
    }
    
    logger.info("APIå¯†é’¥çŠ¶æ€:")
    for provider, available in keys.items():
        logger.info(f"  {provider}: {'âœ…' if available else 'âŒ'}")
    
    # æµ‹è¯•åˆ›å»ºagents
    agents = {}
    
    try:
        agents["æ¨èAgent"] = create_recommendation_agent()
        logger.success("âœ… æ¨èAgentåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨èAgentåˆ›å»ºå¤±è´¥: {e}")
    
    try:
        agents["è¯„ä¼°Agent"] = create_evaluation_agent()
        logger.success("âœ… è¯„ä¼°Agentåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°Agentåˆ›å»ºå¤±è´¥: {e}")
    
    try:
        agents["ä¼˜åŒ–Agent"] = create_optimizer_agent()
        logger.success("âœ… ä¼˜åŒ–Agentåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–Agentåˆ›å»ºå¤±è´¥: {e}")
    
    # ç®€å•æµ‹è¯•
    if agents:
        logger.info("\næµ‹è¯•ç”Ÿæˆ...")
        for name, agent in agents.items():
            try:
                response = agent.generate("Hello, introduce yourself briefly.")
                logger.info(f"{name} å“åº”: {response[:100]}...")
            except Exception as e:
                logger.error(f"{name} æµ‹è¯•å¤±è´¥: {e}")